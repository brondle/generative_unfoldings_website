{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CURSE OF DIMENSIONALITY\n",
    "**Philipp Schmitt, 2020**\n",
    "\n",
    "This notebook contains a workflow to download diagrams from www.arxiv.org, an open-access archive of research papers popular in the machine learning community.\n",
    "\n",
    "**Careful: re-running this notebook will overwrite the diagrams/ folder and image-list.js file, altering the work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "import os\n",
    "import urllib\n",
    "import tarfile\n",
    "import glob\n",
    "import shutil\n",
    "from PIL import Image, ImageStat\n",
    "from wand.image import Image as WandImage\n",
    "import wand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries found: 100\n"
     ]
    }
   ],
   "source": [
    "# Define a query for the arxiv.org search. A few examples:\n",
    "# \"all:latent+space\"\n",
    "# \"all:curse+of+dimensionality\"\n",
    "# \"all:high+dimensional+space\"\n",
    "# \"1209.4915\" (This one will return just a single paper. Useful for testing)\n",
    "query = \"all:'curse of dimensionality'\"\n",
    "\n",
    "# Set http request params\n",
    "start = 0\n",
    "max_results = 100\n",
    "url = \"http://export.arxiv.org/api/query?search_query=%s&start=%d&max_results=%d\" % (query, start, max_results)\n",
    "\n",
    "# Make request and store response xml tree\n",
    "response = requests.get(url)\n",
    "tree = ElementTree.fromstring(response.content)\n",
    "\n",
    "print('entries found:', len(tree.findall('{http://www.w3.org/2005/Atom}entry')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "valid_filetypes = ['png', 'jpg', 'jpeg', 'gif', 'pdf']\n",
    "# folders in which diagrams will be exported. Relative to Jupyter directory\n",
    "download_folder = 'downloads/'\n",
    "dest_folder = 'diagrams/'\n",
    "# I filter diagrams by the image brightness. That way I get mostly figures on white background\n",
    "brightness_thresh = 200\n",
    "# create a list of filenames\n",
    "file_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder if it doesn't exist yet\n",
    "if not os.path.exists(dest_folder):\n",
    "    os.makedirs(dest_folder)\n",
    "        \n",
    "# Iterate through document tree\n",
    "for item in tree.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "    id = item.find('{http://www.w3.org/2005/Atom}id').text.split('/')[-1]\n",
    "    src_url = 'http://arxiv.com/e-print/%s' % id\n",
    "    \n",
    "    # Store the paper and all assets in a downloads folder first.\n",
    "    folder = download_folder + '%s/' % id\n",
    "    file = folder + 'download.tar.gz'\n",
    "    \n",
    "    # Skip download if that paper was already downloaded\n",
    "    # (could have been part of an earlier query)\n",
    "    if os.path.exists(folder):\n",
    "        print('skipped', id)\n",
    "\n",
    "    # Nope, doesn' exist. Download the paper then!\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(src_url, file)\n",
    "        except:\n",
    "            print(id, 'had an error')\n",
    "            continue\n",
    "        print('downloaded', id)\n",
    "\n",
    "        # extract compressed file\n",
    "        try:\n",
    "            tar = tarfile.open(file, \"r:gz\")\n",
    "            tar.extractall(folder)\n",
    "            tar.close()\n",
    "        except: \n",
    "            try:\n",
    "                tar = tarfile.open(file, \"r:\")\n",
    "                tar.extractall(folder)\n",
    "                tar.close()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # iterate over files to select keepers\n",
    "        for i, filename in enumerate(glob.iglob(folder + '**', recursive=True)):\n",
    "            \n",
    "            # first, check if file has approved file type\n",
    "            if filename.split('.')[-1] in valid_filetypes:\n",
    "                filetype = filename.split('/')[-1][-4:]\n",
    "                fn = filename\n",
    "                im = 0\n",
    "    \n",
    "                # convert PDF images to JPG format\n",
    "                if filetype == '.pdf':\n",
    "                    with WandImage(filename=fn, resolution=200) as img:\n",
    "                        img.background_color = wand.color.Color('white')\n",
    "                        img.alpha_channel='remove'\n",
    "                        im = img.save(filename=\"temp.jpg\")\n",
    "                        im = Image.open(\"temp.jpg\").convert('L')\n",
    "                        filetype = '.jpg'\n",
    "                        filename = \"temp.jpg\"\n",
    "                else:\n",
    "                    im = Image.open(filename).convert('L')\n",
    "                \n",
    "                # filter by brightness threshold\n",
    "                brightness = ImageStat.Stat(im).mean[0]\n",
    "                if brightness_thresh <= brightness:\n",
    "                    # generate output filename + rename\n",
    "                    file_out = id + '_%d' % i + filetype\n",
    "                    os.rename(filename, dest_folder + file_out)\n",
    "                    # add file name to list\n",
    "                    if file_out not in file_list :\n",
    "                        file_list.append(file_out)\n",
    "                    \n",
    "        # delete content in download folder; preserve empty folder to check for duplicates later\n",
    "        contents = [os.path.join(folder, i) for i in os.listdir(folder)]\n",
    "        x = [os.remove(i) if os.path.isfile(i) or os.path.islink(i) else shutil.rmtree(i) for i in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File image-list.js created'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, export a .js file with a list of image filenames.\n",
    "file = open(\"image-list.js\", \"w\")\n",
    "file.write('const images = [\"')\n",
    "file.write('\",\"'.join(file_list))\n",
    "file.write('\"];')\n",
    "file.close()\n",
    "\"File image-list.js created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing\n",
    "I go through the `diagrams` folder and manually deleted duplicates and diagrams I didn't like. Then I compress all image files for web using [ImageOptim](https://imageoptim.com/mac)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Giving credit\n",
    "\n",
    "**All diagrams belong to the respective authors and their publications**. Folder names in `downloads/` are named with the arxiv.org publication ID. Image files in `images/` are as well, with the ID ending before the `_n.jpg`. Use the ID to look up the paper on www.arxiv.org\n",
    "\n",
    "On *web scraping*: I've touched on the politics and violence of scraping images for datasets [in this essay](https://humans-of.ai/editorial/). And in journalism, The Markup makes a case for [Why Web Scraping Is Vital to Democracy](https://themarkup.org/news/2020/12/03/why-web-scraping-is-vital-to-democracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
